{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb044f72-bf58-4db8-ae2f-3c258922c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606f5622-31b6-4b49-85a0-a86387d9f3d3",
   "metadata": {
    "id": "606f5622-31b6-4b49-85a0-a86387d9f3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khotv\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e0fbd2-882e-4763-89b8-6f58a212524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c598b8e-b661-4d2d-964f-5cbca7b56b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load config first\n",
    "peft_model_path = \"./saved_model/gpt2-medium-lora-shakespeare/\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5736ec14-e6d3-4904-b8f8-c1c5ab928124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "# Load base model and wrap with PEFT\n",
    "base_model = GPT2LMHeadModel.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49861f4-86c5-4071-a59d-b62e150e7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d33305-9011-4426-be29-420b9d4ce38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c488201b-a298-4a4a-9053-710aa52d0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ot_for_prompt(input_):\n",
    "    inputs = tokenizer(input_, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output (adjust parameters as needed)\n",
    "    output_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=80,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        temperature=0.80,\n",
    "        repetition_penalty=1.15,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Important for GPT\n",
    "    )\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    output_text = clean_repetition(output_text)\n",
    "    print(output_text)\n",
    "    print(\"----------------------------------------------------------------------\")\n",
    "    return output_text\n",
    "\n",
    "def clean_repetition(output):\n",
    "    import re\n",
    "    # print(\"Input type:\", type(output))  # Debug print\n",
    "    output = str(output)\n",
    "    output = re.sub(r'\\b(nobel prize in the )\\1', r'\\1', output, flags=re.IGNORECASE)\n",
    "    output = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', output)\n",
    "    output = re.sub(r'\\b(for )\\1', r'\\1', output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9556eb27-0660-42c3-b732-70ae21eff9b3",
   "metadata": {
    "id": "9556eb27-0660-42c3-b732-70ae21eff9b3"
   },
   "outputs": [],
   "source": [
    "def print_model_stats(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8d42cd-d0ec-4d03-a695-4dfa7dc678ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 355.61M\n"
     ]
    }
   ],
   "source": [
    "print_model_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e7a241-a10a-42fc-b4af-2c485b956f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model evaluation mode added\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"model evaluation mode added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61e339-143c-4986-a6da-f40275a33c7e",
   "metadata": {},
   "source": [
    "### Basic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fb08f-9728-482f-9817-42117d139511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f269066c-7146-432d-abc9-6ecc7858e404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "To a father and to his son.\n",
      "I am come from thee hence, sir, for thy sake; but I will not stay there long. But if thou wouldst leave me an hour, my brother, I must take care that thou wilt give some answer before he comes in at night. So bid him speak of the first day's affairs with me;\n",
      "That\n",
      "----------------------------------------------------------------------\n",
      "hermione:\n",
      "But, if thou be a good friend of mine,\n",
      "I will hear thee well. I can assure thy pardon;--\n",
      "Thou hast now heard me the word's truth and done all\n",
      "for my safety that ever was spoken to me; but how\n",
      "dost speak it? It is not in your nature.\n",
      "And what! are thou willing to do no\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt_lis = [\n",
    "    \"first citizen:\",\n",
    "    \"hermione:\"\n",
    "]\n",
    "\n",
    "\n",
    "for prompt in prompt_lis:\n",
    "    print_ot_for_prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67330c0-96d5-4fed-91ad-2c137c4e4b39",
   "metadata": {},
   "source": [
    "#### More prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4267d30-81c7-4ab0-9368-f815b60158fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT I, SCENE I\n",
      "first citizen:\n",
      "I have heard your words; so do you.\n",
      "What is this?\n",
      "\n",
      "First Citizen : O, no harm in that. To what!\n",
      "So then we will leave town with all our provisions, my lord? and go up the river? which can\n",
      "be done by horse or foot, till any of us shall meet again the\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ACT I, SCENE I\\nfirst citizen:\\nI have heard your words; so do you.\\nWhat is this?\\n\\nFirst Citizen : O, no harm in that. To what!\\nSo then we will leave town with all our provisions, my lord? and go up the river? which can\\nbe done by horse or foot, till any of us shall meet again the'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_ot_for_prompt(\"ACT I, SCENE I\\nfirst citizen:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "302b8d19-900a-4ef8-bdd0-51aa37901255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be ?\"\n",
      "The king, as often as he thinks himself asked in his own mind, answers: \"Now I find it my purpose.\n",
      "I will go myself and look on the queen's son;\n",
      "And see how good she is there!\"--and again goes away with a great shout; 'tis very hard! for this time we must have one more.\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'to be or not to be ?\"\\nThe king, as often as he thinks himself asked in his own mind, answers: \"Now I find it my purpose.\\nI will go myself and look on the queen\\'s son;\\nAnd see how good she is there!\"--and again goes away with a great shout; \\'tis very hard! for this time we must have one more.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_ot_for_prompt(\"to be or not to be \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de98852-0027-43de-ac31-9e46944f1f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3342210-e716-4ea5-bce7-6a4b00942237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f6e4b6-e3ab-4df8-a5b6-13ffeac78155",
   "metadata": {},
   "source": [
    "### Utilizing a Hugging Face hosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e6a2aff-3fb1-4a7d-a99b-606d314d577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT I, SCENE I\n",
      "first citizen:\n",
      "I hope he hath a true faith. O, then, let him make no reply; his eyes are full of hate\n",
      "and grief, and the mind is filled with dread and anger.\n",
      "O, now! how long will that last? He would not be my son-in law,\n",
      "but one's own father: but 'tis\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"khotveer1/gpt2-medium-lora-finetuned-shakespeare\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"khotveer1/gpt2-medium-lora-finetuned-shakespeare\")\n",
    "\n",
    "prompt = \"ACT I, SCENE I\\nfirst citizen:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=80,\n",
    "    do_sample=True,\n",
    "    top_k=40,\n",
    "    temperature=0.80,\n",
    "    repetition_penalty=1.15,\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24b041-d2f2-4c92-a782-26f46f7ca0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31ef1e-878e-4205-92b3-eb53feb54896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7128308,
     "sourceId": 11384149,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm_env1",
   "language": "python",
   "name": "llm_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
